{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbe2b471",
   "metadata": {},
   "source": [
    "Scale and encode the datasets obtained online.\n",
    "Then save the processed datasets as CSV files.\n",
    "One dataset is worked on at a time.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b95247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# 1. LOAD LIBRARIES AND CONFIG\n",
    "# -------------------------------------------\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "\n",
    "# Ensure config module is accessible and up-to-date\n",
    "sys.path.insert(0, \"C:/Misc/binary_eval\")\n",
    "import config\n",
    "importlib.reload(config)\n",
    "\n",
    "# Load config paths\n",
    "from config import PROCESSED_DATA_DIR, METADATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c882a137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# 2. LOAD ONE DATASET\n",
    "# -------------------------------------------\n",
    "\n",
    "# Define the dataset filename to process\n",
    "raw_filename = \"dataset1.csv\"  # Replace with actual filename\n",
    "\n",
    "# Define paths\n",
    "DATASETS_FOR_BENCHMARK = \"D:\\\\Datasets\"    # UPDATE THIS PATH AS NEEDED -----------\n",
    "\n",
    "# Construct full path to the dataset\n",
    "raw_path = os.path.join(DATASETS_FOR_BENCHMARK, raw_filename)\n",
    "\n",
    "# Load dataset\n",
    "try:\n",
    "    df = pd.read_csv(raw_path)\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(f\"‚ùå File not found: {raw_path}\")\n",
    "except pd.errors.ParserError:\n",
    "    raise ValueError(f\"‚ùå Parsing error while reading: {raw_path}\")\n",
    "\n",
    "# Basic metadata\n",
    "num_rows, num_cols = df.shape\n",
    "num_numeric = df.select_dtypes(include=[\"number\"]).shape[1]\n",
    "num_categorical = df.select_dtypes(include=[\"object\", \"category\", \"bool\"]).shape[1]\n",
    "has_target = \"target\" in df.columns\n",
    "\n",
    "# Confirmatory prints\n",
    "print(f\"Loaded dataset: {raw_filename}\")\n",
    "print(f\"Shape: {num_rows:,} rows √ó {num_cols:,} columns\")\n",
    "print(f\"Numeric features: {num_numeric}\")\n",
    "print(f\"Categorical features: {num_categorical}\")\n",
    "print(f\"{'‚úÖ' if has_target else '‚ùå'} 'target' column {'found' if has_target else 'missing'} in dataset\")\n",
    "\n",
    "summary_stats = {}\n",
    "\n",
    "summary_stats['raw_filename'] = raw_filename\n",
    "summary_stats['pipeline_stage'] = \"04_scale_encode_export\"\n",
    "summary_stats['original_row_count'] = num_rows\n",
    "summary_stats['final_row_count'] = num_rows  # No rows dropped yet\n",
    "summary_stats['rows_dropped'] = 0\n",
    "summary_stats['percent_dropped'] = 0.0\n",
    "summary_stats['total_numeric_features'] = num_numeric\n",
    "summary_stats['total_categorical_features'] = num_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a54a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# 3. CHECK FOR TARGET\n",
    "# -------------------------------------------\n",
    "\n",
    "# Validate \"target\" column exists and is binary (0 or 1)\n",
    "\n",
    "if \"target\" not in df.columns:\n",
    "    raise ValueError(\"‚ùå 'target' column is missing from the dataset.\")\n",
    "\n",
    "# Check unique values in target\n",
    "target_values = df[\"target\"].dropna().unique()\n",
    "target_set = set(target_values)\n",
    "\n",
    "# Confirm binary status\n",
    "if target_set == {0, 1} or target_set == {1, 0}:\n",
    "    print(\"‚úÖ 'target' column is binary (0 and 1).\")\n",
    "elif len(target_set) == 1 and target_set.issubset({0, 1}):\n",
    "    print(f\"‚ö†Ô∏è 'target' column contains only one class: {target_set}. Still binary, but may not be useful for modeling.\")\n",
    "else:\n",
    "    raise ValueError(f\"‚ùå 'target' column contains unexpected values: {target_set}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebbce8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# 4. DROP ROWS WITH MISSING VALUES\n",
    "# -------------------------------------------\n",
    "\n",
    "initial_row_count = df.shape[0]\n",
    "df.dropna(inplace=True)\n",
    "final_row_count = df.shape[0]\n",
    "rows_dropped = initial_row_count - final_row_count\n",
    "percent_dropped = (rows_dropped / initial_row_count) * 100 if initial_row_count > 0 else 0\n",
    "\n",
    "# Display message\n",
    "if rows_dropped > 0:\n",
    "    print(f\"Dropped {rows_dropped:,} rows with missing values \"\n",
    "          f\"({percent_dropped:.2f}% of total).\")\n",
    "else:\n",
    "    print(\"‚úÖ No missing values found. No rows dropped.\")\n",
    "\n",
    "# Log to summary dictionary\n",
    "\n",
    "summary_stats['rows_dropped'] = rows_dropped\n",
    "summary_stats['percent_dropped'] = round(percent_dropped, 2)\n",
    "summary_stats['final_row_count'] = final_row_count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcad693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# 5. SCALE NUMERIC FEATURES\n",
    "# -------------------------------------------\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Identify numeric columns (excluding 'target')\n",
    "numeric_cols = df.select_dtypes(include=[\"number\"]).columns.drop(\"target\", errors=\"ignore\")\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = df.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns\n",
    "\n",
    "# Initialize scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "# First check if there are any numeric features to scale\n",
    "# If none, skip all scaling and give message.\n",
    "if len(numeric_cols) > 0:\n",
    "    df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "    print(f\"Scaled {len(numeric_cols)} numeric feature(s) using StandardScaler.\")\n",
    "else:\n",
    "    print(\"No numeric columns found to scale.\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Scaled {len(numeric_cols)} numeric feature(s) using StandardScaler.\")\n",
    "print(f\"Left {len(categorical_cols)} categorical feature(s) and the 'target' column untouched.\")\n",
    "\n",
    "summary_stats['total_numeric_features'] = len(numeric_cols)\n",
    "summary_stats['numeric_features_scaled'] = len(numeric_cols)\n",
    "summary_stats['total_categorical_features'] = len(categorical_cols)\n",
    "summary_stats['scaling_method'] = \"StandardScaler\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af783dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# 6. PRE-ENCODING DIAGNOSTIC ‚Äî CATEGORICAL EXPANSION CHECK\n",
    "# -------------------------------------------\n",
    "\n",
    "# Estimate column expansion for each categorical feature\n",
    "print(\"üîç Pre-encoding diagnostic ‚Äî estimated column expansion per categorical feature:\\n\")\n",
    "\n",
    "for col in categorical_cols:\n",
    "    n_unique = df[col].nunique(dropna=False)\n",
    "    print(f\"‚Ä¢ '{col}': {n_unique} unique value(s) ‚Üí {n_unique} potential column(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dcc73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# 7. DELETE SELECTED FEATURES FROM PRE-ENCODED DATAFRAME\n",
    "# -------------------------------------------\n",
    "\n",
    "# Provides a means to delete unneeded features before encoding (e.g., a sequentially assigned ID)\n",
    "# Of course, simply removing them mannually from the dataset also works.\n",
    "\n",
    "# Paste in the list of features to delete.\n",
    "features_to_delete = []  # E.g., ['feature1', 'feature2']\n",
    "\n",
    "# Confirm which features are present before deletion\n",
    "print(\"Features before deletion:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Drop specified features\n",
    "df = df.drop(columns=features_to_delete, errors='ignore')\n",
    "\n",
    "# Confirm deletion\n",
    "print(\"\\nDeleted features:\")\n",
    "print(features_to_delete)\n",
    "\n",
    "print(\"\\nRemaining features:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Update summary_stats with deleted features\n",
    "summary_stats['features_deleted'] = features_to_delete\n",
    "summary_stats['num_features_deleted'] = len(features_to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d3a074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# 8. ENCODE CATEGORICAL FEATURES\n",
    "# -------------------------------------------\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = df.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n",
    "\n",
    "# Dictionary to track number of new columns per feature\n",
    "encoding_summary = {}\n",
    "\n",
    "# Encode each categorical feature individually\n",
    "for col in categorical_cols:\n",
    "    encoded = pd.get_dummies(df[col], prefix=col, drop_first=False)\n",
    "    df.drop(columns=[col], inplace=True)\n",
    "    df = pd.concat([df, encoded], axis=1)\n",
    "    encoding_summary[col] = encoded.shape[1]\n",
    "    print(f\"üî£ Encoded '{col}' ‚Üí {encoded.shape[1]} new column(s)\")\n",
    "\n",
    "# Total new columns added\n",
    "total_new_columns = sum(encoding_summary.values())\n",
    "print(f\"\\nüßæ Total new columns created from encoding: {total_new_columns:,}\")\n",
    "\n",
    "# Track encoded categorical features for downstream use\n",
    "encoded_categorical_features = list(encoding_summary.keys())\n",
    "\n",
    "# Final output: copy-paste friendly list of encoded features\n",
    "print(\"\\nüìã Encoded categorical features (for deletion or review):\")\n",
    "print(\", \".join(f'\"{feature}\"' for feature in encoded_categorical_features))\n",
    "\n",
    "print(f\"\\nThe number of remaining numeric features: {len(numeric_cols)}\")\n",
    "\n",
    "# Print remaining numeric feature names (max 8 per row)\n",
    "print(\"\\nüî¢ Remaining numeric features:\")\n",
    "for i in range(0, len(numeric_cols), 8):\n",
    "    print(\", \".join(numeric_cols[i:i+8]))\n",
    "\n",
    "# Update summary_stats with encoding details\n",
    "summary_stats['encoded_categorical_features'] = encoded_categorical_features\n",
    "summary_stats['total_encoded_columns'] = total_new_columns\n",
    "summary_stats['remaining_numeric_features'] = numeric_cols.tolist()\n",
    "summary_stats['num_remaining_numeric_features'] = len(numeric_cols)\n",
    "summary_stats['encoding_method'] = \"OneHot (drop_first=False)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3ed395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# 10. BUILD SUMMARY STATISTICS DICTIONARY IN PREFERRED ORDER\n",
    "# -------------------------------------------\n",
    "\n",
    "pipeline_stage = \"04_scale_encode_export\"\n",
    "\n",
    "summary_stats = {\n",
    "    \"raw_filename\": raw_filename,\n",
    "    \"pipeline_stage\": pipeline_stage,\n",
    "    \"original_row_count\": num_rows,\n",
    "    \"final_row_count\": final_row_count,\n",
    "    \"rows_dropped\": rows_dropped,\n",
    "    \"percent_dropped\": percent_dropped,\n",
    "    \"total_numeric_features\": len(numeric_cols),\n",
    "    \"numeric_features_scaled\": len(numeric_cols),\n",
    "    \"scaling_method\": \"StandardScaler\",\n",
    "    \"total_categorical_features\": len(categorical_cols),\n",
    "    \"encoding_method\": \"OneHot (drop_first=False)\",\n",
    "    \"encoded_categorical_features\": encoded_categorical_features,\n",
    "    \"total_encoded_columns\": total_new_columns,\n",
    "    \"features_deleted\": features_to_delete,\n",
    "    \"num_features_deleted\": len(features_to_delete),\n",
    "    \"remaining_numeric_features\": numeric_cols.tolist(),\n",
    "    \"num_remaining_numeric_features\": len(numeric_cols)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd614280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# 9. SAVE OUTPUTS FOR USE BY NEXT MODULE\n",
    "# -------------------------------------------\n",
    "\n",
    "# 1. Save transformed dataframe as Parquet\n",
    "transformed_path = PROCESSED_DATA_DIR / f\"{raw_filename.replace('.csv', '')}_transformed.parquet\"\n",
    "\n",
    "df.to_parquet(transformed_path)\n",
    "print(f\"Saved transformed dataframe to: {transformed_path}\")\n",
    "\n",
    "# 2. Save transformation metadata as JSON\n",
    "metadata_path = METADATA_DIR / f\"{raw_filename.replace('.csv', '')}_transform_metadata.json\"\n",
    "with open(metadata_path, \"w\") as f:\n",
    "    json.dump(summary_stats, f, indent=2)\n",
    "print(f\"Saved transformation metadata to: {metadata_path}\")\n",
    "\n",
    "# 3. Print new shape and column count\n",
    "print(f\"\\nTransformed shape: {df.shape}\")\n",
    "print(f\"Total columns after transform: {df.shape[1]:,}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
