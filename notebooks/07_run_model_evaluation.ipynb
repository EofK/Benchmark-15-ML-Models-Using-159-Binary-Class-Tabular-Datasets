{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1081a3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# 1. LOAD LIBRARIES AND CONFIG\n",
    "# -------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# For model evaluation\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "import importlib\n",
    "\n",
    "\n",
    "# Import config paths so they are accessible to module.\n",
    "import sys\n",
    "sys.path.insert(0, \"C:/Misc/binary_eval\")\n",
    "import config\n",
    "import importlib\n",
    "importlib.reload(config)    # Reload config to ensure latest edits are active\n",
    "\n",
    "from config import RESULTS_DIR, PROCESSED_DATA_DIR, OUTPUTS_DIR, TUNED_MODELS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d7b4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# 2. SELECT AND LOAD TUNING METADATA\n",
    "# -------------------------------------------\n",
    "\n",
    "# Obtain selected model and dataset names set in a previous module\n",
    "with open(OUTPUTS_DIR / \"last_selection.json\", \"r\") as f:\n",
    "    selection = json.load(f)\n",
    "selected_model = selection[\"selected_model\"]\n",
    "dataset_name = selection[\"dataset_name\"]\n",
    "\n",
    "results_path = TUNED_MODELS_DIR / f\"{selected_model}_{dataset_name}_tuning_results.json\"\n",
    "\n",
    "# Load the JSON file containing best parameters and metadata\n",
    "with open(results_path, \"r\") as f:\n",
    "    tuning_metadata = json.load(f)\n",
    "\n",
    "# Print parameters used to instantiate the model - mrerely for confirmation\n",
    "best_params = tuning_metadata[\"best_params\"]\n",
    "print(\"Parameters used to instantiate model:\")\n",
    "print(best_params)\n",
    "\n",
    "# Merge all_params with best_params so all required parameters are present.\n",
    "# This ensures that scikit_learn won't use its own defaults for missing parameters,\n",
    "# including solver, which defaults to lbfgs (even though liblinear is defined\n",
    "# originally as the default solver).\n",
    "params = {**tuning_metadata.get(\"all_params\", {}), **best_params}\n",
    "\n",
    "# Instantiate the model and print its parameters\n",
    "ModelClass = getattr(importlib.import_module(tuning_metadata[\"model_module\"]), tuning_metadata[\"model_name\"])\n",
    "model = ModelClass(**params)\n",
    "print(\"Model parameters after instantiation:\")\n",
    "print(model.get_params())\n",
    "\n",
    "\n",
    "grid_runtime = tuning_metadata[\"grid_runtime\"]\n",
    "param_combinations = tuning_metadata[\"param_combinations\"]\n",
    "\n",
    "print(f\"Loaded tuning metadata from: {results_path}\")\n",
    "print(\"Model name:\", tuning_metadata[\"model_name\"] )\n",
    "print(\"Best parameters:\", tuning_metadata[\"best_params\"] )\n",
    "print(f\"Grid search runtime: {grid_runtime:,.4f} seconds\")\n",
    "print(f\"Number of parameter combinations: {tuning_metadata['param_combinations']:,}\")\n",
    "\n",
    "print(\"Here are tuned parameters\")\n",
    "print(model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc37f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# 3. CONFIRM PATHS\n",
    "# -------------------------------------------\n",
    "\n",
    "print(\"PROCESSED_DATA_DIR:\", PROCESSED_DATA_DIR)\n",
    "\n",
    "\n",
    "dataset_path = PROCESSED_DATA_DIR / f\"{dataset_name}_transformed.parquet\"\n",
    "print(\"Dataset path:\", dataset_path)\n",
    "print(\"Exists:\", dataset_path.exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66360e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# 4. SELECT AND LOAD DATASET\n",
    "# -------------------------------------------\n",
    "\n",
    "\n",
    "# Build the path to the processed parquet file\n",
    "dataset_path = PROCESSED_DATA_DIR / f\"{dataset_name}_transformed.parquet\"\n",
    "\n",
    "# Load the processed dataset\n",
    "df = pd.read_parquet(dataset_path)\n",
    "\n",
    "print(f\"Loaded dataset: {dataset_path}\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62dc4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# 5. INSTANTIATE MODEL WITH TUNED PARAMETERS - newest ver\n",
    "# -------------------------------------------\n",
    "\n",
    "# The model to instantiate in this code cell is determined\n",
    "# by the values in the tuning_metadata dictionary\n",
    "# which was loaded from the tuned model JSON file read\n",
    "# in a previous code cell of this module: 2. SELECT AND \n",
    "# LOAD TUNING METADATA.\n",
    "\n",
    "# Dynamically import the model class\n",
    "model_module = tuning_metadata[\"model_module\"] if \"model_module\" in tuning_metadata else \"sklearn.ensemble\"\n",
    "model_class_name = tuning_metadata[\"model_name\"]\n",
    "best_params = tuning_metadata[\"best_params\"]\n",
    "\n",
    "# Merge all_params with best_params so all required parameters are present\n",
    "params = {**tuning_metadata.get(\"all_params\", {}), **best_params}\n",
    "\n",
    "\n",
    "# Import the model class dynamically\n",
    "ModelClass = getattr(importlib.import_module(model_module), model_class_name)\n",
    "\n",
    "# Instantiate the model with robust parameters.\n",
    "# If best_params were used, sckikit will default to lbfgs (not wanted).\n",
    "model = ModelClass(**params)\n",
    "\n",
    "print(f\"Instantiated {model_class_name} with all these parameters:\")\n",
    "print(params)\n",
    "print()\n",
    "print(f\"Tuned parameters (best_params):\")\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbb90bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# 6. SPLIT FEATURES AND TARGET\n",
    "# -------------------------------------------\n",
    "\n",
    "# Specify the target column\n",
    "target_col = \"target\"   # All datasets for this benchmark use \"target\".\n",
    "\n",
    "# Split the dataset into features (X) and target (y)\n",
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col]\n",
    "\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Target value counts:\\n{y.value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2822facf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# 6a. SANITIZE COLUMN NAMES BECAUSE ONE-HOT ENCODING CAN CREATE PROBLEMS\n",
    "# -------------------------------------------\n",
    "\n",
    "# Sanitize column names if the model crashes because of column names.\n",
    "# This code was added only because LGBMClassifier crashed on one dataset.\n",
    "\n",
    "# Sanitize column names after one-hot encoding to prevent model crashes.\n",
    "# LightGBM and similar libraries require feature names to be ASCII, printable, and free of control characters.\n",
    "# One-hot encoding creates new columns using raw categorical values, which may contain spaces, punctuation,\n",
    "# Unicode, or invisible/control characters. Sanitizing ensures robust, error-free model training.\n",
    "\n",
    "# Problematic characters in column names that could cause model crashes include:\n",
    "#  1. Invisible Unicode characters (e.g., zero-width space, non-breaking space)\n",
    "#  2. Non-ASCII symbols\n",
    "#  3. Hidden formatting or control characters\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "def sanitize_column_names(columns):\n",
    "    # Remove any character that is not a letter, digit, or underscore\n",
    "    return [re.sub(r'[^\\w]', '_', str(col)) for col in columns]\n",
    "\n",
    "# Apply to features\n",
    "X.columns = sanitize_column_names(X.columns)\n",
    "print(\"Sanitized columns:\", X.columns.tolist())\n",
    "\n",
    "for col in X.columns:\n",
    "    for c in col:\n",
    "        if ord(c) < 32 or ord(c) > 126:\n",
    "            print(f\"Non-printable character found in column: {col!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f43ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# 7. EVALUATE MODEL: CROSS-VALIDATION\n",
    "# -------------------------------------------\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "# Set up StratifiedKFold\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Populate evaluation metrics\n",
    "metrics = {\n",
    "    \"accuracy\": \"accuracy\",\n",
    "    \"f1_weighted\": \"f1_weighted\",\n",
    "    \"roc_auc\": \"roc_auc_ovr\"\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for metric_name, scoring in metrics.items():\n",
    "    scores = cross_val_score(model, X, y, cv=cv, scoring=scoring, n_jobs=-1)\n",
    "    results[metric_name] = {\n",
    "        \"mean\": scores.mean(),\n",
    "        \"std\": scores.std(),\n",
    "        \"all_scores\": scores.tolist()\n",
    "    }\n",
    "\n",
    "# Print results\n",
    "for metric_name, stats in results.items():\n",
    "    print(f\"{metric_name.capitalize()} - Mean: {stats['mean']:.4f}, Std: {stats['std']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc51543b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# 8. AUTOMATE PARAMS METADATA COLUMN\n",
    "# -------------------------------------------\n",
    "\n",
    "# Build params_metadata using the tuned parameters and param grid keys if available.\n",
    "# The stringified params will be one of the fields recorded for this model's results\n",
    "# in an Excel file that is capturing the results of each model-dataset pair.\n",
    "\n",
    "if \"param_grid\" in tuning_metadata:\n",
    "    param_grid_keys = list(tuning_metadata[\"param_grid\"].keys())\n",
    "else:\n",
    "    param_grid_keys = list(best_params.keys())   # fallback to best_params keys\n",
    "\n",
    "# Use best_params for tuned run\n",
    "params_metadata = {key: best_params.get(key, None) for key in param_grid_keys}\n",
    "\n",
    "print(\"Params metadata for benchmark run:\")\n",
    "print(params_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c350cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# 9. MEASURE PEAK RAM USAGE AND THROUGHPUT FOR PREDICTION ONLY\n",
    "# -------------------------------------------\n",
    "\n",
    "import time\n",
    "import psutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Show model being evaluated.\n",
    "print(f\"Evaluating this model: {model}\")\n",
    "print()\n",
    "\n",
    "n_loops = 5\n",
    "max_ram_mb = 0\n",
    "total_predictions = 0\n",
    "total_pred_runtime = 0.0\n",
    "\n",
    "process = psutil.Process()\n",
    "\n",
    "# Split data for prediction benchmarking\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the model once before prediction loops\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "for i in range(n_loops):\n",
    "    start_ram = process.memory_info().rss / (1024 ** 2)  # MB\n",
    "    pred_start = time.time()\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    pred_end = time.time()\n",
    "    ram_after_pred = process.memory_info().rss / (1024 ** 2)  # MB\n",
    "    max_ram_mb = max(max_ram_mb, start_ram, ram_after_pred)\n",
    "    n_preds = len(X_test)\n",
    "    total_predictions += n_preds\n",
    "    loop_pred_runtime = pred_end - pred_start\n",
    "    total_pred_runtime += loop_pred_runtime\n",
    "\n",
    "# Calculate throughput for prediction only\n",
    "throughput = total_predictions / total_pred_runtime if total_pred_runtime > 0 else 0\n",
    "\n",
    "print(f\"Peak RAM usage during prediction over {n_loops} loops: {max_ram_mb:.2f} MB\")\n",
    "print(f\"Total predictions made: {total_predictions:,.2f}\")\n",
    "print(f\"Total cumulative prediction runtime: {total_pred_runtime:.4f} seconds\")\n",
    "print(f\"Prediction throughput (predictions/sec): {throughput:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df34412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# 10. RECORD RESULTS TO EXCEL\n",
    "# -------------------------------------------\n",
    "\n",
    "\n",
    "# Prepare results row with all required metadata\n",
    "results_row = {\n",
    "    \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"model_name\": model_class_name,\n",
    "    \"dataset\": dataset_name,\n",
    "    \"n_samples\": X.shape[0],\n",
    "    \"n_features\": X.shape[1],\n",
    "    \"fit_loops\": n_loops,\n",
    "    \"params\": str(params_metadata),\n",
    "    \"accuracy_mean\": results[\"accuracy\"][\"mean\"],\n",
    "    \"accuracy_std\": results[\"accuracy\"][\"std\"],\n",
    "    \"f1_weighted_mean\": results[\"f1_weighted\"][\"mean\"],\n",
    "    \"f1_weighted_std\": results[\"f1_weighted\"][\"std\"],\n",
    "    \"auc_mean\": results[\"roc_auc\"][\"mean\"],\n",
    "    \"auc_std\": results[\"roc_auc\"][\"std\"],\n",
    "    \"runtime_total\": total_pred_runtime,\n",
    "    \"peak_ram_mb\": max_ram_mb,\n",
    "    \"throughput\": throughput,\n",
    "    \"grid_runtime\": grid_runtime,\n",
    "    \"param_combinations\": param_combinations\n",
    "}\n",
    "\n",
    "# Save to the Excel that has one row for the results of each model-dataset pair\n",
    "excel_path = RESULTS_DIR / \"model_results_benchmark.xlsx\"\n",
    "excel_path.parent.mkdir(parents=True, exist_ok=True)  # Ensure directory exists\n",
    "\n",
    "\n",
    "\n",
    "df_results = pd.DataFrame([results_row])\n",
    "if excel_path.exists():\n",
    "    existing = pd.read_excel(excel_path)\n",
    "    df_results = pd.concat([existing, df_results], ignore_index=True)\n",
    "\n",
    "df_results.to_excel(excel_path, index=False)\n",
    "\n",
    "print(f\"Results saved to: {excel_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
