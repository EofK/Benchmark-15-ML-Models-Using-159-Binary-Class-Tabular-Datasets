{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "237c2489",
   "metadata": {},
   "source": [
    "Develop and save to an Excel file various metrics about a dataset.\n",
    "\n",
    "Two sets of metrics are defined:\n",
    "\n",
    "1. Various measures that are not includedin Python's problexity (e.g., Kurtosis and skewness)\n",
    "2. The 22 Lorena et al. complexity measures produced by Python's problexity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377f06c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# 1. LOAD LIBRARIES AND CONFIG\n",
    "# -------------------------------------------\n",
    "\n",
    "\n",
    "# Import config paths so they are accessible to module.\n",
    "import sys\n",
    "sys.path.insert(0, \"C:/pathnaame\")\n",
    "import config\n",
    "import importlib\n",
    "importlib.reload(config)    # Reload config to ensure latest edits are active"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fdbcf5",
   "metadata": {},
   "source": [
    "The two code cells that follow generate and record measures for each dataset, and then creates and save an Excel file with the results, one row for each dataset.\n",
    "\n",
    "The first module defines the functions to develop individual dataset measures that are not the dataset complexity measures defined by Lorena et al. and generated by problexity.  This first module defines a wrapper function that is then called to generate all of the individual non-Lorena et al. dataset measures.\n",
    "\n",
    "The second module runs the wrapper function and a problexity code block inside a loop that reads all datasets in the identified path. The problexity module calculates the 22 Lorena et al. complexity measures (plus a combined complexity \"score\"). The module creates and saves an Excel file of the results, one row for each dataset.  Each Excel worksheet column is one of the dataset measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29af54c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# 2. DEFINE FUNCTIONS TO DEVELOP METRICS OF A DATASET\n",
    "# -------------------------------------------\n",
    "\n",
    "# The functions below define individual dataset metrics (metadata) that are not generated by problexity.\n",
    "# These functions then are called from the wrapper function defined at the bottom of this section.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_shape_info(df: pd.DataFrame) -> dict:\n",
    "    \"\"\"Return number of rows and columns (columns include the target).\"\"\"\n",
    "    n_samples, n_features = df.shape\n",
    "    return {\"n_samples\": n_samples, \"n_features\": n_features}\n",
    "\n",
    "def get_feature_types(df: pd.DataFrame, target_col: str) -> dict:\n",
    "    \"\"\"Count numeric vs categorical features (excluding the target).\"\"\"\n",
    "    X = df.drop(columns=[target_col])\n",
    "    numeric_cols = X.select_dtypes(include=\"number\").columns.tolist()\n",
    "    categorical_cols = X.select_dtypes(exclude=\"number\").columns.tolist()\n",
    "    return {\n",
    "        \"n_numeric_features\": len(numeric_cols),\n",
    "        \"n_categorical_features\": len(categorical_cols),\n",
    "        # returned for convenience if you need them downstream\n",
    "        \"numeric_columns\": numeric_cols,\n",
    "        \"categorical_columns\": categorical_cols,\n",
    "    }\n",
    "\n",
    "def get_class_imbalance(df: pd.DataFrame, target_col: str) -> dict:\n",
    "    \"\"\"Return class counts and majority/minority ratio.\"\"\"\n",
    "    counts = df[target_col].value_counts(dropna=False)\n",
    "    if counts.empty or len(counts) == 1:\n",
    "        imbalance = float(\"inf\")  # or None, if you prefer\n",
    "    else:\n",
    "        imbalance = counts.max() / counts.min()\n",
    "    return {\n",
    "        \"class_counts\": counts.to_dict(),\n",
    "        \"class_imbalance\": float(imbalance),\n",
    "    }\n",
    "\n",
    "\n",
    "# variance_near_zero_count function\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def variance_near_zero_count(df: pd.DataFrame, eps: float = 1e-12) -> dict:\n",
    "\n",
    "    # Exclude target; operate on numeric features only\n",
    "    X_num = df.drop(columns=[\"target\"], errors=\"ignore\").select_dtypes(include=\"number\")\n",
    "    if X_num.shape[1] == 0:\n",
    "        return {\"variance_near_zero_count\": 0, \"near_zero_columns\": []}\n",
    "\n",
    "    # Variance with NaN-safe behavior (treat all-NaN as zero variance)\n",
    "    variances = X_num.var(axis=0, ddof=0).fillna(0.0)\n",
    "    near_zero_cols = variances.index[variances <= eps].tolist()\n",
    "\n",
    "    return {\n",
    "        \"variance_near_zero_count\": len(near_zero_cols),\n",
    "        \"near_zero_columns\": near_zero_cols\n",
    "    }\n",
    "\n",
    "\n",
    "# Unique Categorical Values function: \n",
    "# Find the categorical feature with the highest number of unique values \n",
    "\n",
    "def max_unique_categorical_values(df: pd.DataFrame, target_col: str) -> dict:\n",
    "\n",
    "    # Exclude target; operate on categorical features only\n",
    "    X_cat = df.drop(columns=[target_col], errors=\"ignore\").select_dtypes(include=[\"object\", \"category\"])\n",
    "    if X_cat.shape[1] == 0:\n",
    "        return {\"max_unique_values\": 0, \"max_unique_column\": None}\n",
    "\n",
    "    # Find the column with the maximum number of unique values\n",
    "    unique_counts = X_cat.nunique()\n",
    "    max_unique_col = unique_counts.idxmax()\n",
    "    max_unique_val = unique_counts.max()\n",
    "\n",
    "    return {\n",
    "        \"max_unique_values\": max_unique_val,\n",
    "        \"max_unique_column\": max_unique_col\n",
    "    }\n",
    "\n",
    "\n",
    "# Sparsity Percent:  Measure the fraction of zero entries in the\n",
    "# numeric features of the dataset, expressed as a percentage.\n",
    "\n",
    "def sparsity_percent(df: pd.DataFrame, target_col: str) -> float:\n",
    "    \"\"\"Calculate the sparsity percentage of numeric features.\"\"\"\n",
    "    # Exclude target; operate on numeric features only\n",
    "    X_num = df.drop(columns=[target_col], errors=\"ignore\").select_dtypes(include=\"number\")\n",
    "    if X_num.shape[1] == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # Calculate sparsity\n",
    "    total_entries = X_num.size\n",
    "    zero_entries = (X_num == 0).sum().sum()\n",
    "    sparsity = zero_entries / total_entries * 100\n",
    "\n",
    "    return sparsity\n",
    "\n",
    "\n",
    "# A revised Sparsity Percent:  Measure the fraction of zero entries in the\n",
    "# numeric features of the dataset, but then adjust for the \n",
    "# categorical features, which are 100% full of data.\n",
    "\n",
    "def adjusted_sparsity_percent(df: pd.DataFrame, target_col: str) -> float:\n",
    "    \"\"\"Calculate adjusted sparsity percentage including categorical features.\"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    # Separate numeric and categorical features\n",
    "    X = df.drop(columns=[target_col], errors=\"ignore\")\n",
    "    X_num = X.select_dtypes(include=\"number\")\n",
    "    X_cat = X.select_dtypes(include=[\"object\", \"category\"])\n",
    "\n",
    "    # Count zero entries in numeric features\n",
    "    zero_entries = (X_num == 0).sum().sum()\n",
    "\n",
    "    # Total entries across all features\n",
    "    total_entries = X_num.size + X_cat.shape[0] * X_cat.shape[1]\n",
    "\n",
    "    if total_entries == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # Adjusted sparsity\n",
    "    adjusted_sparsity = zero_entries / total_entries * 100\n",
    "    return round(adjusted_sparsity, 4)\n",
    "\n",
    "\n",
    "\n",
    "#  n_onehot_features:  Determine the number of expanded columns \n",
    "#  that would be created by one-hot. To calculate this:  For each categorical \n",
    "#  feature: (a) Count the number of unique values (cardinality), then\n",
    "#  (b) Sum these counts across all categorical features.\n",
    "\n",
    "\n",
    "def n_onehot_features(df: pd.DataFrame, target_col: str) -> int:\n",
    "\n",
    "    # Exclude target; operate on categorical features only\n",
    "    X_cat = df.drop(columns=[target_col], errors=\"ignore\").select_dtypes(include=[\"object\", \"category\"])\n",
    "    if X_cat.shape[1] == 0:\n",
    "        return 0\n",
    "\n",
    "    # Sum the number of unique values across all categorical features\n",
    "    total_onehot_features = X_cat.nunique().sum()\n",
    "\n",
    "    return total_onehot_features\n",
    "\n",
    "\n",
    "#  Total features after encoding:  Determine total number of \n",
    "#  features there would be after encoding all categorical features.  \n",
    "#  Determined as sum of Number of Numerical Features and n-onehot_features.\n",
    "\n",
    "def total_features_after_encoding(df: pd.DataFrame, target_col: str) -> int:\n",
    "\n",
    "    n_numeric = df.drop(columns=[target_col], errors=\"ignore\").select_dtypes(include=\"number\").shape[1]\n",
    "    n_onehot = n_onehot_features(df, target_col=target_col)\n",
    "    return n_numeric + n_onehot\n",
    "\n",
    "\n",
    "#  feature_cardinality_stats:  Determine the min/median/max \n",
    "#  unique values per (original) categorical feature. Store as a string \n",
    "#  in this field (e.g., feature_cardinality_stats = \"min=2, median=8, max=30000\")\n",
    "\n",
    "def feature_cardinality_stats(df: pd.DataFrame, target_col: str) -> str:\n",
    "\n",
    "    # Exclude target; operate on categorical features only\n",
    "    X_cat = df.drop(columns=[target_col], errors=\"ignore\").select_dtypes(include=[\"object\", \"category\"])\n",
    "    if X_cat.shape[1] == 0:\n",
    "        return \"min=0, median=0, max=0\"\n",
    "\n",
    "    unique_counts = X_cat.nunique()\n",
    "    min_unique = unique_counts.min()\n",
    "    median_unique = int(unique_counts.median())\n",
    "    max_unique = unique_counts.max()\n",
    "\n",
    "    return f\"min={min_unique}, median={median_unique}, max={max_unique}\"\n",
    "\n",
    "\n",
    "#  Number of Categorical Features with more than n (e.g., 50)\n",
    "#  unique values: Determine how many of the categorical features \n",
    "#  have more than min_num_of_unique_values unique values.\n",
    "def n_categorical_features_gt50(df: pd.DataFrame, target_col: str, threshold: int = 50) -> int:\n",
    "\n",
    "    # Exclude target; operate on categorical features only\n",
    "    X_cat = df.drop(columns=[target_col], errors=\"ignore\").select_dtypes(include=[\"object\", \"category\"])\n",
    "    if X_cat.shape[1] == 0:\n",
    "        return 0\n",
    "\n",
    "    # Count features with unique values greater than the threshold\n",
    "    count_gt_threshold = int((X_cat.nunique() > threshold).sum())\n",
    "\n",
    "    return count_gt_threshold\n",
    "\n",
    "\n",
    "# Mean Absolute Correlation: Calculate the mean of the absolute Pearson\n",
    "# correlations among numeric features (excluding self-correlations).\n",
    "\n",
    "def mean_abs_corr(df: pd.DataFrame, target_col: str) -> float:\n",
    "\n",
    "    # Exclude target; operate on numeric features only\n",
    "    X_num = df.drop(columns=[target_col], errors=\"ignore\").select_dtypes(include=\"number\")\n",
    "    # Drop columns with zero variance\n",
    "    X_num = X_num.loc[:, X_num.var(axis=0, ddof=0) > 0]\n",
    "    if X_num.shape[1] < 2:\n",
    "        return 0.0  # Not enough numeric features to compute correlations\n",
    "    corr_matrix = X_num.corr().abs()\n",
    "    # Exclude self-correlations by masking the diagonal\n",
    "    mask = ~np.eye(corr_matrix.shape[0], dtype=bool)\n",
    "    mean_corr = np.nanmean(corr_matrix.values[mask])\n",
    "    return 1 / (mean_corr + 1e-6)  # Inverted for complexity (higher is more complex)\n",
    "\n",
    "\n",
    "# Calculate the share of redundant features\n",
    "\n",
    "def correlation_redundancy(df: pd.DataFrame, target_col: str, threshold: float = 0.95) -> float:\n",
    "\n",
    "    # Exclude target; operate on numeric features only\n",
    "    X_num = df.drop(columns=[target_col], errors=\"ignore\").select_dtypes(include=\"number\")\n",
    "    # Drop columns with zero variance\n",
    "    X_num = X_num.loc[:, X_num.var(axis=0, ddof=0) > 0]\n",
    "    n = X_num.shape[1]\n",
    "    if n < 2:\n",
    "        return 0.0  # Not enough numeric features to compute correlations\n",
    "    corr_matrix = X_num.corr().abs()\n",
    "    # Exclude self-correlations by masking the diagonal\n",
    "    mask = ~np.eye(n, dtype=bool)\n",
    "    corr_vals = corr_matrix.values[mask]\n",
    "    # Count pairs with correlation >= threshold\n",
    "    n_pairs = len(corr_vals)\n",
    "    n_redundant = np.sum(corr_vals >= threshold)\n",
    "    redundancy_ratio = float(n_redundant) / n_pairs if n_pairs > 0 else 0.0\n",
    "    return 1 / (redundancy_ratio + 1e-6)    #Inverted for complexity (higher is more complex)\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the mean mutual information of the top-10 features with the label\n",
    "from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
    "\n",
    "def mi_top10_mean(df, target_col: str, task_type: str = \"classification\") -> float:\n",
    "\n",
    "    # Exclude target; operate on numeric features only\n",
    "    X_num = df.drop(columns=[target_col], errors=\"ignore\").select_dtypes(include=\"number\")\n",
    "    y = df[target_col]\n",
    "    # Drop columns with zero variance\n",
    "    X_num = X_num.loc[:, X_num.var(axis=0, ddof=0) > 0]\n",
    "    if X_num.shape[1] == 0:\n",
    "        return 0.0\n",
    "    # Choose MI function based on task type\n",
    "    if task_type == \"classification\":\n",
    "        mi = mutual_info_classif(X_num, y, discrete_features=\"auto\")\n",
    "    else:\n",
    "        mi = mutual_info_regression(X_num, y)\n",
    "    # Get top-10 MI values\n",
    "    top10 = sorted(mi, reverse=True)[:10]\n",
    "    if len(top10) == 0:\n",
    "        return 0.0\n",
    "    return float(np.mean(top10))\n",
    "\n",
    "\n",
    "# Calculate the mean mutual information score for all numeric feature pairs (excluding the target)\n",
    "# Must use mutual_info_regression for this function b/c continuous numeric features.\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "import numpy as np\n",
    "\n",
    "def mean_mi_score(df, target_col: str) -> float:\n",
    "\n",
    "    # Exclude target; operate on numeric features only\n",
    "    X_num = df.drop(columns=[target_col], errors=\"ignore\").select_dtypes(include=\"number\")\n",
    "    if X_num.shape[1] < 2:\n",
    "        return 0.0\n",
    "    mi_scores = []\n",
    "    for i, col1 in enumerate(X_num.columns):\n",
    "        for j, col2 in enumerate(X_num.columns):\n",
    "            if i < j:\n",
    "                # MI between col1 and col2 (both continuous)\n",
    "                mi = mutual_info_regression(X_num[[col1]], X_num[col2])\n",
    "                mi_scores.append(float(mi[0]))\n",
    "    if len(mi_scores) == 0:\n",
    "        return 0.0\n",
    "    return float(np.mean(mi_scores))\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the mean overall class-wise mutual information score for all features\n",
    "# Must use mutual_info_classification for this function b/c of categorical features.\n",
    "# Drop rows with NaNs to avoid errors.\n",
    "\n",
    "def mean_mutual_info(df: pd.DataFrame, target_col: str) -> float:\n",
    "    from sklearn.feature_selection import mutual_info_classif\n",
    "    from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "    # Separate features and target\n",
    "    X = df.drop(columns=[target_col], errors=\"ignore\")\n",
    "    y = df[target_col]\n",
    "\n",
    "    # Encode categorical features\n",
    "    X_encoded = X.copy()\n",
    "    cat_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "    if len(cat_cols) > 0:\n",
    "        X_encoded[cat_cols] = OrdinalEncoder().fit_transform(X[cat_cols])\n",
    "\n",
    "    # Drop rows with any NaNs\n",
    "    valid_mask = X_encoded.notna().all(axis=1) & y.notna()\n",
    "    X_encoded = X_encoded[valid_mask]\n",
    "    y = y[valid_mask]\n",
    "\n",
    "    # Safety check: if no rows remain, return 0.0\n",
    "    if X_encoded.shape[0] == 0 or X_encoded.shape[1] == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # Compute mutual information\n",
    "    mi = mutual_info_classif(X_encoded, y, discrete_features='auto')\n",
    "    mean_mi = float(np.mean(mi))\n",
    "    return round(1 / (mean_mi + 1e-6), 4)   # Inverted for complexity (higher is more complex)\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the mean class-wise mutual information score for the top 5 features\n",
    "# Drop NaNs to avoid errors.\n",
    "\n",
    "def top5_mean_mutual_info(df: pd.DataFrame, target_col: str) -> float:\n",
    "    from sklearn.feature_selection import mutual_info_classif\n",
    "    from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "    # Separate features and target\n",
    "    X = df.drop(columns=[target_col], errors=\"ignore\")\n",
    "    y = df[target_col]\n",
    "\n",
    "    # Encode categorical features\n",
    "    X_encoded = X.copy()\n",
    "    cat_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "    if len(cat_cols) > 0:\n",
    "        X_encoded[cat_cols] = OrdinalEncoder().fit_transform(X[cat_cols])\n",
    "\n",
    "    # Drop rows with any NaNs\n",
    "    valid_mask = X_encoded.notna().all(axis=1) & y.notna()\n",
    "    X_encoded = X_encoded[valid_mask]\n",
    "    y = y[valid_mask]\n",
    "\n",
    "    # Safety check: if no rows remain, return 0.0\n",
    "    if X_encoded.shape[0] == 0 or X_encoded.shape[1] == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # Compute mutual information\n",
    "    mi = mutual_info_classif(X_encoded, y, discrete_features='auto')\n",
    "\n",
    "    # Compute top-5 mean MI\n",
    "    top5_mean = np.mean(np.sort(mi)[-5:]) if len(mi) >= 5 else np.mean(mi)\n",
    "    return round(1 / (top5_mean + 1e-6), 4)   # Inverted for complexity (higher is more complex)\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the class-wise mutual information score for the single feature with the highest MI\n",
    "# Drop NaNs to avoid errors.\n",
    "\n",
    "def max_mutual_info(df: pd.DataFrame, target_col: str) -> float:\n",
    "    from sklearn.feature_selection import mutual_info_classif\n",
    "    from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "    # Separate features and target\n",
    "    X = df.drop(columns=[target_col], errors=\"ignore\")\n",
    "    y = df[target_col]\n",
    "\n",
    "    # Encode categorical features\n",
    "    X_encoded = X.copy()\n",
    "    cat_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "    if len(cat_cols) > 0:\n",
    "        X_encoded[cat_cols] = OrdinalEncoder().fit_transform(X[cat_cols])\n",
    "\n",
    "    # Drop rows with any NaNs\n",
    "    valid_mask = X_encoded.notna().all(axis=1) & y.notna()\n",
    "    X_encoded = X_encoded[valid_mask]\n",
    "    y = y[valid_mask]\n",
    "\n",
    "    # Safety check: if no rows remain, return 0.0\n",
    "    if X_encoded.shape[0] == 0 or X_encoded.shape[1] == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # Compute mutual information\n",
    "    mi = mutual_info_classif(X_encoded, y, discrete_features='auto')\n",
    "    max_mi = float(np.max(mi))\n",
    "    return round(1 / (max_mi + 1e-6), 4)   # Inverted for complexity (higher is more complex)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Mean Kurtosis: Calculate the average kurtosis across all numeric features\n",
    "from scipy.stats import kurtosis\n",
    "import numpy as np\n",
    "\n",
    "def mean_kurtosis(df, target_col: str) -> float:\n",
    "\n",
    "    # Exclude target; operate on numeric features only\n",
    "    X_num = df.drop(columns=[target_col], errors=\"ignore\").select_dtypes(include=\"number\")\n",
    "    if X_num.shape[1] == 0:\n",
    "        return 0.0\n",
    "    # Calculate kurtosis for each numeric feature (axis=0: columns)\n",
    "    kurt_vals = kurtosis(X_num, axis=0, nan_policy=\"omit\", fisher=True)\n",
    "    # Replace nan with 0 for mean calculation\n",
    "    kurt_vals = np.nan_to_num(kurt_vals)\n",
    "    return float(np.mean(kurt_vals))\n",
    "\n",
    "\n",
    "# Calculate mean absolute skewness for all numeric features\n",
    "\n",
    "from scipy.stats import skew\n",
    "import numpy as np\n",
    "\n",
    "def mean_abs_skewness(df, target_col: str) -> float:\n",
    "\n",
    "    # Exclude target; operate on numeric features only\n",
    "    X_num = df.drop(columns=[target_col], errors=\"ignore\").select_dtypes(include=\"number\")\n",
    "    if X_num.shape[1] == 0:\n",
    "        return 0.0\n",
    "    # Calculate skewness for each numeric feature (axis=0: columns)\n",
    "    skew_vals = skew(X_num, axis=0, nan_policy=\"omit\")\n",
    "    # Take absolute value and replace nan with 0 for mean calculation\n",
    "    abs_skew_vals = np.abs(skew_vals)\n",
    "    abs_skew_vals = np.nan_to_num(abs_skew_vals)\n",
    "    return float(np.mean(abs_skew_vals))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Wrapper Function (calls the functions above) ---\n",
    "# To make higher values of each dataset metric indicate a higher dataset complexity,\n",
    "# some metrics are inverted (e.g., class imbalance becomes 1 / class imbalance).\n",
    "\n",
    "def dataset_metadata(df: pd.DataFrame, target_col: str, file_path: str = None, infrastructure_info: str = None) -> dict:\n",
    "    \"\"\"\n",
    "    Minimal version: bundles the first three metrics (shape, feature types, class imbalance).\n",
    "    Add more fields here later as you implement additional helpers.\n",
    "    \"\"\"\n",
    "    meta = {}\n",
    "\n",
    "    # 1) shape\n",
    "    meta.update(get_shape_info(df))\n",
    "\n",
    "    # 2) feature types\n",
    "    ft = get_feature_types(df, target_col=target_col)\n",
    "    meta.update({\n",
    "        \"n_numeric_features\": ft[\"n_numeric_features\"],\n",
    "        \"n_categorical_features\": ft[\"n_categorical_features\"],\n",
    "    })\n",
    "\n",
    "\n",
    "    # 3) class imbalance\n",
    "    meta.update(get_class_imbalance(df, target_col=target_col))\n",
    "\n",
    "    # 4) variance near zero\n",
    "    vnz = variance_near_zero_count(df)\n",
    "    meta.update({\n",
    "        \"variance_near_zero_count\": vnz[\"variance_near_zero_count\"]\n",
    "    })\n",
    "\n",
    "    # 5) max unique categorical values\n",
    "    mucv = max_unique_categorical_values(df, target_col=target_col)\n",
    "    meta.update({\n",
    "        \"max_unique_values\": int(mucv[\"max_unique_values\"]),\n",
    "        \"max_unique_column\": mucv[\"max_unique_column\"]\n",
    "    })\n",
    "\n",
    "    # 6) sparsity percent\n",
    "    meta[\"sparsity_percent\"] = float(sparsity_percent(df, target_col=target_col))\n",
    "\n",
    "    # 6b) adjusted sparsity percent (includes categorical features)\n",
    "    meta[\"adjusted_sparsity_percent\"] = float(adjusted_sparsity_percent(df, target_col=target_col))\n",
    "    \n",
    "    # 7) n_onehot_features\n",
    "    meta[\"n_onehot_features\"] = int(n_onehot_features(df, target_col=target_col))\n",
    "    \n",
    "    # 8) total_features_after_encoding\n",
    "    meta[\"total_features_after_encoding\"] = int(total_features_after_encoding(df, target_col=target_col))\n",
    "\n",
    "    # 9) feature_cardinality_stats\n",
    "    meta[\"feature_cardinality_stats\"] = feature_cardinality_stats(df, target_col=target_col)\n",
    "\n",
    "    # 10) n_categorical_features_gt50\n",
    "    meta[\"n_categorical_features_gt50\"] = n_categorical_features_gt50(df, target_col=target_col, threshold=50)\n",
    "\n",
    "    # 11) max absolute correlation\n",
    "    meta[\"max_absolute_correlation_inv\"] = float(max_abs_corr(df, target_col=target_col))\n",
    "    \n",
    "    # 12) mean absolute correlation\n",
    "    meta[\"mean_absolute_correlation_inv\"] = float(mean_abs_corr(df, target_col=target_col))\n",
    "    \n",
    "    # 13) correlation redundancy\n",
    "    meta[\"correlation_redundancy_inv\"] = float(correlation_redundancy(df, target_col=target_col, threshold=0.95))\n",
    "\n",
    "    # 14) Skipped\n",
    "\n",
    "    # 15) Skipped\n",
    "\n",
    "    # 16) mean class-wise mutual information (all features)\n",
    "    meta[\"mean_classwise_mutual_information_inv\"] = float(mean_mutual_info(df, target_col=target_col))\n",
    "\n",
    "    # 17) mean class-wise mutual information (top-5 features)\n",
    "    meta[\"top5_mean_classwise_mutual_information_inv\"] = float(top5_mean_mutual_info(df, target_col=target_col))\n",
    "\n",
    "    # 18) max class-wise mutual information (single feature)\n",
    "    meta[\"max_classwise_mutual_information_inv\"] = float(max_mutual_info(df, target_col=target_col))\n",
    "\n",
    "    # 19) mean kurtosis\n",
    "    meta[\"mean_kurtosis\"] = float(mean_kurtosis(df, target_col=target_col))\n",
    "\n",
    "    # 20) mean absolute skewness\n",
    "    meta[\"mean_absolute_skewness\"] = float(mean_abs_skewness(df, target_col=target_col))\n",
    "\n",
    "    \n",
    "\n",
    "    # Optional fields that are included:\n",
    "    if file_path is not None:\n",
    "        try:\n",
    "            import os\n",
    "            meta[\"file_size_bytes\"] = os.path.getsize(file_path)\n",
    "        except Exception:\n",
    "            meta[\"file_size_bytes\"] = None\n",
    "    if infrastructure_info is not None:\n",
    "        meta[\"infrastructure_info\"] = infrastructure_info\n",
    "\n",
    "    return meta\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2477cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# 3. CYCLE THROUGH DATASETS AND POPULATE EXCEL\n",
    "# -------------------------------------------\n",
    "\n",
    "# For each dataset in the DATASETS_PATH folder, load it, compute the metadata and problexity metrics,\n",
    "# and append to the manifest (Excel file).\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler  # Add this at the top\n",
    "import time\n",
    "import problexity as px\n",
    "\n",
    "\n",
    "# Define paths\n",
    "DATASETS_PATH = Path(r\"C:\\Folder_Name_Here\")    # Path to datasets\n",
    "EXCEL_MANIFEST_PATH = Path(r\"C:\\Second_Folder_Here\\filename.xlsx\")    # Name the Excel file here\n",
    "\n",
    "\n",
    "# Infrastructure descriptor is a column in the manifest (Excel file)\n",
    "infrastructure_text = \"Local Win 11 /Python 3.13 / 32GB RAM / Intel Core i7-14700F processor (20 cores, 28 logical processors, 2.10 GHz base speed), 32 GB of RAM\"\n",
    "\n",
    "\n",
    "column_order = [\n",
    "    \"timestamp\",\n",
    "    \"dataset_name\",\n",
    "    \"infrastructure_info\",\n",
    "\n",
    "    \"n_samples\",                             # 1. Number of Records\n",
    "    \"n_features\",                            # 2. Number of Features\n",
    "    \"n_numeric_features\",                    # 3. Number of Numerical Features\n",
    "    \"n_categorical_features\",                # 4. Number of Categorical Features\n",
    "    \"max_unique_values\",                     # 5. Maximum Number of Unique Categorical Values\n",
    "    \"max_unique_column\",                     # 6. Maximum Number of Unique Column Name\n",
    "    \"n_onehot_features\",                     # 7. One-hot Number of Features\n",
    "    \"total_features_after_encoding\",         # 8. Total Number of Features After Encoding\n",
    "    \"feature_cardinality_stats\",             # 9. Cardinality Summary\n",
    "    \"n_categorical_features_gt50\",           # 10. Categorical Features > 50 Unique Values\n",
    "    \"class_imbalance\",                       # 11. Class Imbalance Ratio\n",
    "    \"sparsity_percent\",                      # 12. Sparsity Percent\n",
    "    \"adjusted_sparsity_percent\",             # 13. Adjusted Sparsity Percent - include categoricals\n",
    "    \"max_absolute_correlation_inv\",          # 14. Maximum Absolute Correlation Inverted\n",
    "    \"mean_absolute_correlation_inv\",         # 15. Mean Absolute Correlation Inverted\n",
    "    \"correlation_redundancy_inv\",            # 16. Correlation Redundancy Inverted\n",
    "    \"mean_mutual_information\",               # 17. Mutual Information Top-10 Mean\n",
    "    \"mean_mutual_information_all\",           # 18. Mean Mutual Information Score (All Pairs)\n",
    "    \"variance_near_zero_count\",              # 19. Near-Zero Variance Count\n",
    "    \"mean_kurtosis\",                         # 20. Mean Kurtosis\n",
    "    \"mean_absolute_skewness\",                # 21. Mean Absolute Skewness\n",
    "    \"mean_classwise_mutual_information_inv\",        # 22. Class-wise Mean Mutual Information Inverted\n",
    "    \"top5_mean_classwise_mutual_information_inv\",   # 23. Class-wise Top-5 Mean Mutual Information Inverted\n",
    "    \"max_classwise_mutual_information_inv\",         # 24. Class-wise Max Mutual Information Inverted\n",
    "    \"score\",                                 # 25. Total Complexity Score (Score)\n",
    "    \"f1\",                                   # 26. Maximum Fisher's Discriminant Ratio (F1)\n",
    "    \"f1v\",                                   # 27. Directional Vector Maximum Fisher’s Discriminant Ratio (F1v)\n",
    "    \"f2\",                                   # 28. Volume of Overlapping Region (F2)\n",
    "    \"f3\",                                    # 29. Maximum Individual Feature Efficiency (F3)\n",
    "    \"f4\",                                   # 30. Collective Feature Efficiency (F4)    \n",
    "    \"l1\",                                   # 31. Sum of the Error Distance by Linear Programming (L1)\n",
    "    \"l2\",                                   # 32. Error Rate of Linear Classifier (L2)\n",
    "    \"l3\",                                   # 33. Non-linearity of Linear Classifier (L3)\n",
    "    \"n1\",                                   # 34. Fraction of Borderline Points (N1)\n",
    "    \"n2\",                                   # 35. Ratio of Intra/Extra NN Distance (N2)\n",
    "    \"n3\",                                   # 36. Error Rate of NN classifier (N3)\n",
    "    \"n4\",                                   # 37. Non linearity of NN (N4)\n",
    "    \"t1\",                                   # 38. Fraction of Hyperspheres Covering Data (T1)\n",
    "    \"lsc\",                                  # 39. Local Set Average Cardinality (LSC)\n",
    "    \"density\",                              # 40. Density (Density)        \n",
    "    \"clsCoef\",                              # 41. Clusering Coefficient (ClsCoef)\n",
    "    \"hubs\",                                 # 42. Hubs (Hubs)\n",
    "    \"t2\",                                   # 43. Average Number of Features Per Point (T2)\n",
    "    \"t3\",                                   # 44. Average Number of PCA Dimensions per Points (T3)\n",
    "    \"t4\",                                   # 45. Ratio of the PCA Dimension to the Original Dimension (T4)\n",
    "    \"c1\",                                   # 46. Entropy of Classes Proportion (C1)\n",
    "    \"c2\",                                   # 47. Imbalance Ratio (C2)\n",
    "    \"file_size_bytes\",                      # 48. File Size\n",
    "    \"class_counts\",                         # 49. Target Class Distribution\n",
    "    \"processing_time_seconds\",              # 50. Time taken to process dataset\n",
    "]\n",
    "\n",
    "# Placeholder for results\n",
    "manifest_rows = []\n",
    "num_files = len(list(DATASETS_PATH.glob(\"*.csv\")))\n",
    "count = 0\n",
    "\n",
    "# Loop through datasets\n",
    "for dataset_file in DATASETS_PATH.glob(\"*.csv\"):\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        df = pd.read_csv(dataset_file)      \n",
    "\n",
    "        # Separate features and target\n",
    "        X = df.drop(columns=[\"target\"])\n",
    "        y = df[\"target\"]\n",
    "\n",
    "        # Drop non-numeric features beacause because problexity is incapabable of\n",
    "        # recognizing and ignoring/dropping categorical features.\n",
    "        X = X.select_dtypes(include=[\"number\"])\n",
    "\n",
    "        # Scale numeric features only. Reduces overflow issues for some \n",
    "        # dataset complexity measure calculations. Also, improves the actual measurement.\n",
    "        numeric_cols = X.select_dtypes(include=[\"number\"]).columns\n",
    "        scaler = StandardScaler()\n",
    "        X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "        # Recombine scaled features with target\n",
    "        df_scaled = X.copy()\n",
    "        df_scaled[\"target\"] = y  \n",
    "\n",
    "        # Extract metrics using fixed target column\n",
    "        # For dataset_metadata function, pass the original df,containing all features.\n",
    "        metadata = dataset_metadata(df, target_col=\"target\")\n",
    "\n",
    "        # Drop NaNs because problexity is incapable of dealing with missing values\n",
    "        # For problexity, must use the df_scaled, which has only numeric features.\n",
    "        df_scaled = df_scaled.dropna()\n",
    "\n",
    "        # Cap dataset size to avoid memory overflow caused by problexity's inefficiencies.\n",
    "        # This is stratified random sampling, ensuring each class gets an equal share of the total row cap.\n",
    "        # To suppress FutureWarning messages, included group_keys=False, include_groups=True\n",
    "        MAX_ROWS = 25000\n",
    "        if len(df_scaled) > MAX_ROWS:\n",
    "            df_scaled = (\n",
    "                df_scaled.groupby(\"target\", group_keys=False)\n",
    "                .apply(lambda x: x.sample(min(len(x), MAX_ROWS // df_scaled[\"target\"].nunique()), random_state=42))\n",
    "                .reset_index(drop=True)\n",
    "            )  \n",
    "\n",
    "\n",
    "        # Run problexity on scaled data\n",
    "        cc = px.ComplexityCalculator()\n",
    "        cc.fit(df_scaled.drop(columns=[\"target\"]), df_scaled[\"target\"])\n",
    "        report = cc.report()\n",
    "\n",
    "        # Merge problexity metrics into metadata\n",
    "        metadata[\"score\"] = report[\"score\"]\n",
    "        metadata.update(report[\"complexities\"])\n",
    "\n",
    "\n",
    "        # Add admin fields\n",
    "        metadata[\"timestamp\"] = datetime.now().isoformat(timespec=\"seconds\")\n",
    "        metadata[\"dataset_name\"] = dataset_file.stem\n",
    "        metadata[\"infrastructure_info\"] = infrastructure_text\n",
    "        # Insert placeholder MI scores - MI calcs crash routine if too many features\n",
    "        metadata[\"mean_mutual_information\"] = 0.0\n",
    "        metadata[\"mean_mutual_information_all\"] = 0.0\n",
    "        metadata[\"file_size_bytes\"] = dataset_file.stat().st_size\n",
    "\n",
    "        # Capture how long it took to develop the dataset metrics\n",
    "        end_time = time.time()\n",
    "        metadata[\"processing_time_seconds\"] = round(end_time - start_time, 2)\n",
    "\n",
    "        # Align to column order, and build the Excel row's dictionary\n",
    "        row = {col: metadata.get(col, None) for col in column_order}      \n",
    "\n",
    "        manifest_rows.append(row)\n",
    "        count += 1\n",
    "        print(f\"✅ Appended {count} of {num_files} files: {dataset_file.name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Skipped {dataset_file.name}: {e}\")\n",
    "\n",
    "# Create DataFrame from new rows\n",
    "manifest_df = pd.DataFrame(manifest_rows)\n",
    "\n",
    "\n",
    "# If Excel file exists, read existing sheet and append\n",
    "if EXCEL_MANIFEST_PATH.exists():\n",
    "    try:\n",
    "        existing_df = pd.read_excel(EXCEL_MANIFEST_PATH, sheet_name=\"Benchmark Manifest\")        \n",
    "        combined_df = pd.concat([existing_df, manifest_df], ignore_index=True)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Could not read existing manifest: {e}\")\n",
    "        combined_df = manifest_df\n",
    "else:\n",
    "    combined_df = manifest_df\n",
    "\n",
    "# Reorder columns to match desired order\n",
    "combined_df = combined_df[column_order]\n",
    "\n",
    "\n",
    "# Overwrite the existing file (i.e., no appending), use default mode (write mode)\n",
    "with pd.ExcelWriter(EXCEL_MANIFEST_PATH, engine=\"openpyxl\") as writer:\n",
    "    combined_df.to_excel(writer, sheet_name=\"Benchmark Manifest\", index=False)\n",
    "\n",
    "\n",
    "print(\"Manifest updated successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
